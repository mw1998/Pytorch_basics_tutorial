{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Don't overfit a sigle batch\n",
    "### 2. Forgot to set model.eval() or model.train()\n",
    "### 3. Forgot to set optimizer.zero_grad()\n",
    "### 4. Using softmax in model with CrossEntropyLoss()\n",
    "### 5. Don't set bais=False when using batchnorm()\n",
    "### 6. Using view as permute\n",
    "### 7. Using bad data augmentation\n",
    "### 8. Not Shuffling Data\n",
    "### 9. Not Normalizing data\n",
    "### 10. Not Clipping gradients (when using RNNs, GRUs, or LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F # relu, tanh\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建随机数种子\n",
    "seed=0\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fully Connected Network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fully Connected Network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 14_How To Save Time\n",
    "        # 4.Don't use softmax with crossentropy\n",
    "        # 在计算交叉熵损失时，已经进行了一次softmax操作，再计算的负样本\n",
    "        # 如果在网络最后的输出层也是softmax，就相当于进行了两次softmax操作\n",
    "        # 可能会带来梯度消失问题\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cnn network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cnn network with batchnorm\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            # 14_How To Save Time\n",
    "            # 5.Set bias=False, when using BatcnNorm\n",
    "            # 使用batchnorm时，bias无效，具体见batchnorm公式\n",
    "            bias=False,\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(784, 10)\n",
    "x = torch.randn(64, 784)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_class = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256 \n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14_How To Save Time\n",
    "# 9. Not normalizing data\n",
    "# 在数据进行训练前，应该进行标准化处理\n",
    "# 所有图像的像素值除以255，使得数据在0-1之间\n",
    "# 进而得到数据集的所有图像的像素的均值和方差\n",
    "# 如果是RGB图像，需要3个通道的均值和方差\n",
    "# 对于灰度图，如MNIST数据，只需要单个均值和方差\n",
    "my_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True) \n",
    "# 14_How to Save Time\n",
    "# 8. Not Shuffling Data\n",
    "# 需要注意的是，在使用时序相关的数据时不会使用shuffle  \n",
    "# 但在大多是情况下使用shuffle都是应该的\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Using my_transform with normalize\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True,\n",
    "                               transform=my_transform,\n",
    "                               download=True) \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False,\n",
    "                               transform=my_transform,\n",
    "                               download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][0].reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input type (torch.cuda.FloatTensor) and weight \n",
    "# type (torch.FloatTensor) should be the same\n",
    "model = CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "model = NN(input_size=input_size, num_classes=num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# 14_How To Save Time\n",
    "# 4.Don't use softmax with crossentropy\n",
    "# 在计算交叉熵损失时，已经进行了一次softmax操作，再计算的负样本\n",
    "# 如果在网络最后的输出层也是softmax，就相当于进行了两次softmax操作\n",
    "# 可能会带来梯度消失问题\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14_How To Save Time\n",
    "# 1. Use overfit a single batch\n",
    "# 在正式训练前使用一个batch的数据进行测试，调整batchsize的大小\n",
    "\n",
    "data, targets = next(iter(train_loader))\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    data = data.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "    scores = model(data)\n",
    "    loss = criterion(scores, targets)\n",
    "    print(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "check_accuracy(test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "# For NN datashape --> 784\n",
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\", end='')\n",
    "    for data, targets in tqdm(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        # 14_How To Save Time\n",
    "        # Don't forget optimizer.zero_grad()\n",
    "        # 在不使用zero_grad()时，优化器用的是过去所有batch梯度的累计\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\", end='')\n",
    "    for data, targets in tqdm(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 14_How To Save Time\n",
    "        # 10. Not clipping gradient (when using RNNs, GRUs, LSTMs)\n",
    "        # 具体原因晓不得，反正说加了这一行，效果会很好\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:07<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:04<00:00, 49.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:04<00:00, 48.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "# For CNN datashape --> 1 x 28 x 28\n",
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\", end='')\n",
    "    for data, targets in tqdm(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        # 14_How To Save Time\n",
    "        # Don't forget optimizer.zero_grad()\n",
    "        # 在不使用zero_grad()时，优化器用的是过去所有batch梯度的累计\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    # model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if loader.dataset.train:\n",
    "            print(\"Checking accuracy on training data\")\n",
    "        else:\n",
    "            print(\"Checking accuracy on test data\")\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x) # shape: 64x10\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions==y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        acc = round(float(num_correct)/float(num_samples)*100, 2)\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy \\\n",
    "              {acc}')\n",
    "    \n",
    "    # model.train()\n",
    "    # return acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test data\n",
      "Got 9789 / 10000 with accuracy               97.89\n"
     ]
    }
   ],
   "source": [
    "# check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14_How To Save Time\n",
    "# 2. Forgot to set training or eval\n",
    "# model.training() 模式下会使用dropout,并只对一个batch做normalization\n",
    "# model.eval() 模式下不会使用dropout，并对整体测试集数据做normalization\n",
    "model.train()\n",
    "check_accuracy(test_loader, model)\n",
    "model.eval()\n",
    "check_accuracy(test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 14_How To Save Time\n",
    "# 6. Using view as permute\n",
    "# view()是按照元素顺序直接重新排列\n",
    "# permute()是置换原矩阵的指定维度\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x)\n",
    "\n",
    "print(x.view(3, 2))\n",
    "print(x.permute(1, 0)) # This is transpose, in specify dimention.\n",
    "print(x.T) # Transpose of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14_How To Save Time\n",
    "# 7. Using bad data augmentation\n",
    "# 在使用数据增强时应考虑训练数据的真实分布情况\n",
    "# 明白数据增强真的达到了你想要的效果\n",
    "# 比如对于MNIST数据集，手写数字识别\n",
    "# 如果做了图像翻转，9的图像变成了6，但标签还是9\n",
    "# 这样会给模型带去误解\n",
    "# 数据增强是好的，但并不是所有数据增强都是好的！！！"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bea151495e21b09669b579db65fdfd1623fa1cb64926cd75da5b5ecb0779b291"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
